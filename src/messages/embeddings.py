"""Semantic search using local embedding models.

This module provides embedding-based semantic search capabilities using
sentence-transformers models that run entirely on device. No data is sent
to external services.

The embeddings enable:
- Semantic similarity search (find messages with similar meaning)
- Search that understands synonyms and related concepts
- Better handling of questions and natural language queries

Usage:
    from messages import get_db
    from messages.embeddings import EmbeddingIndex

    db = get_db()
    index = EmbeddingIndex()
    index.build(db)  # This may take a while on first run

    # Semantic search - finds conceptually similar messages
    results = index.search("What time should we meet?")
    # Might find "Let's meet at noon" even though no exact word matches

Models:
    The default model is 'all-MiniLM-L6-v2' which provides a good balance of
    speed and quality. It's about 90MB and runs efficiently on CPU.
"""

import json
import sqlite3
import struct
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Iterator, Optional

from .models import apple_time_to_datetime

if TYPE_CHECKING:
    from .db import MessagesDB


# Try to import sentence-transformers
try:
    from sentence_transformers import SentenceTransformer

    EMBEDDINGS_AVAILABLE = True
except ImportError:
    SentenceTransformer = None
    EMBEDDINGS_AVAILABLE = False


# Default model - small and efficient
DEFAULT_MODEL = "all-MiniLM-L6-v2"


def get_default_embedding_index_path() -> Path:
    """Get the default path for the embedding index.

    Uses ~/Library/Application Support/macos-messages/ on macOS.
    """
    app_support = Path.home() / "Library" / "Application Support" / "macos-messages"
    app_support.mkdir(parents=True, exist_ok=True)
    return app_support / "embedding_index.db"


def _serialize_embedding(embedding: list[float]) -> bytes:
    """Serialize embedding vector to bytes for storage.

    Uses struct for efficient binary storage.
    """
    return struct.pack(f"{len(embedding)}f", *embedding)


def _deserialize_embedding(data: bytes) -> list[float]:
    """Deserialize embedding vector from bytes."""
    count = len(data) // 4  # 4 bytes per float
    return list(struct.unpack(f"{count}f", data))


def _cosine_similarity(a: list[float], b: list[float]) -> float:
    """Compute cosine similarity between two vectors.

    Returns value between -1 and 1, with 1 being most similar.
    """
    dot_product = sum(x * y for x, y in zip(a, b))
    norm_a = sum(x * x for x in a) ** 0.5
    norm_b = sum(x * x for x in b) ** 0.5

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return dot_product / (norm_a * norm_b)


@dataclass
class SemanticSearchResult:
    """A semantic search result with similarity score."""

    message_id: int
    chat_id: int
    text: str
    date: datetime
    is_from_me: bool
    similarity: float  # Cosine similarity score (0-1, higher is more similar)


class EmbeddingIndex:
    """Embedding-based semantic search index.

    This index stores message embeddings generated by a local transformer model,
    enabling semantic similarity search across messages.
    """

    def __init__(
        self,
        path: Optional[Path | str] = None,
        model_name: str = DEFAULT_MODEL,
    ):
        """Initialize the embedding index.

        Args:
            path: Path to the index database. Defaults to app data directory.
            model_name: Name of the sentence-transformers model to use.
        """
        self.path = Path(path) if path else get_default_embedding_index_path()
        self.model_name = model_name
        self._conn: Optional[sqlite3.Connection] = None
        self._model: Optional["SentenceTransformer"] = None

    @property
    def conn(self) -> sqlite3.Connection:
        """Get database connection, creating if needed."""
        if self._conn is None:
            self._conn = sqlite3.connect(str(self.path))
            self._conn.row_factory = sqlite3.Row
            self._init_schema()
        return self._conn

    @property
    def model(self) -> "SentenceTransformer":
        """Get the embedding model, loading if needed."""
        if not EMBEDDINGS_AVAILABLE:
            raise RuntimeError(
                "sentence-transformers not installed. "
                "Install with: pip install sentence-transformers"
            )

        if self._model is None:
            self._model = SentenceTransformer(self.model_name)

        return self._model

    def _init_schema(self) -> None:
        """Create the index schema if it doesn't exist."""
        cursor = self.conn.cursor()

        # Table to store message embeddings
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS message_embeddings (
                message_id INTEGER PRIMARY KEY,
                chat_id INTEGER NOT NULL,
                date INTEGER NOT NULL,
                is_from_me INTEGER NOT NULL,
                text TEXT NOT NULL,
                embedding BLOB NOT NULL
            )
        """)

        # Index for faster lookups
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_embeddings_date
            ON message_embeddings(date)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_embeddings_chat
            ON message_embeddings(chat_id)
        """)

        # Metadata table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS embedding_metadata (
                key TEXT PRIMARY KEY,
                value TEXT
            )
        """)

        # Store model info
        cursor.execute("""
            INSERT OR REPLACE INTO embedding_metadata (key, value)
            VALUES ('model_name', ?)
        """, (self.model_name,))

        self.conn.commit()

    def _get_indexed_count(self) -> int:
        """Get the number of indexed messages."""
        cursor = self.conn.execute("SELECT COUNT(*) FROM message_embeddings")
        return cursor.fetchone()[0]

    def _get_embedding_dimension(self) -> Optional[int]:
        """Get the dimension of stored embeddings."""
        cursor = self.conn.execute(
            "SELECT embedding FROM message_embeddings LIMIT 1"
        )
        row = cursor.fetchone()
        if row:
            return len(_deserialize_embedding(row["embedding"]))
        return None

    def build(
        self,
        db: "MessagesDB",
        *,
        full_rebuild: bool = False,
        batch_size: int = 100,
        progress_callback: Optional[callable] = None,
    ) -> int:
        """Build or update the embedding index from the Messages database.

        Args:
            db: MessagesDB instance to read messages from
            full_rebuild: If True, rebuild the entire index from scratch
            batch_size: Number of messages to embed at once (for efficiency)
            progress_callback: Optional callback(indexed, total) for progress updates

        Returns:
            Number of messages indexed
        """
        if not EMBEDDINGS_AVAILABLE:
            raise RuntimeError(
                "sentence-transformers not installed. "
                "Install with: pip install sentence-transformers"
            )

        if full_rebuild:
            self._clear_index()

        # Get all message IDs already indexed
        cursor = self.conn.execute("SELECT message_id FROM message_embeddings")
        indexed_ids = {row[0] for row in cursor}

        # Query all messages from the Messages database
        messages_cursor = db.conn.execute("""
            SELECT m.ROWID, m.text, m.attributedBody, m.date, m.is_from_me,
                   cmj.chat_id
            FROM message m
            JOIN chat_message_join cmj ON m.ROWID = cmj.message_id
            WHERE (m.associated_message_type IS NULL OR m.associated_message_type = 0)
              AND (m.text IS NOT NULL AND m.text != '')
            ORDER BY m.date ASC
        """)

        # Collect messages to index
        messages_to_index = []
        for row in messages_cursor:
            message_id = row["ROWID"]

            # Skip already indexed messages
            if message_id in indexed_ids:
                continue

            text = row["text"]
            if not text:
                # Try to extract from attributedBody
                from .db import _extract_text_from_attributed_body
                text = _extract_text_from_attributed_body(row["attributedBody"])

            if not text or len(text.strip()) < 3:  # Skip very short messages
                continue

            messages_to_index.append({
                "message_id": message_id,
                "chat_id": row["chat_id"],
                "date": row["date"],
                "is_from_me": row["is_from_me"],
                "text": text.strip(),
            })

        if not messages_to_index:
            return 0

        # Index in batches for efficiency
        indexed_count = 0
        for i in range(0, len(messages_to_index), batch_size):
            batch = messages_to_index[i:i + batch_size]
            self._index_batch(batch)
            indexed_count += len(batch)

            if progress_callback:
                progress_callback(indexed_count, len(messages_to_index))

        return indexed_count

    def _index_batch(self, messages: list[dict]) -> None:
        """Index a batch of messages."""
        # Get embeddings for all texts in the batch
        texts = [msg["text"] for msg in messages]
        embeddings = self.model.encode(texts, convert_to_numpy=True)

        cursor = self.conn.cursor()

        for msg, embedding in zip(messages, embeddings):
            # Store message with embedding
            cursor.execute(
                """INSERT INTO message_embeddings
                   (message_id, chat_id, date, is_from_me, text, embedding)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (msg["message_id"], msg["chat_id"], msg["date"],
                 msg["is_from_me"], msg["text"],
                 _serialize_embedding(embedding.tolist()))
            )

        self.conn.commit()

    def _clear_index(self) -> None:
        """Clear the entire index."""
        cursor = self.conn.cursor()
        cursor.execute("DELETE FROM message_embeddings")
        self.conn.commit()

    def search(
        self,
        query: str,
        *,
        chat_id: Optional[int] = None,
        chat_ids: Optional[list[int]] = None,
        after: Optional[datetime] = None,
        before: Optional[datetime] = None,
        limit: int = 20,
        min_similarity: float = 0.3,
    ) -> Iterator[SemanticSearchResult]:
        """Search for semantically similar messages.

        Args:
            query: Natural language search query
            chat_id: Limit search to specific chat
            chat_ids: Limit search to multiple chats
            after: Only messages after this date
            before: Only messages before this date
            limit: Maximum results (default 20)
            min_similarity: Minimum similarity score (0-1, default 0.3)

        Yields:
            SemanticSearchResult objects ordered by similarity (highest first)
        """
        if not EMBEDDINGS_AVAILABLE:
            raise RuntimeError(
                "sentence-transformers not installed. "
                "Install with: pip install sentence-transformers"
            )

        from .models import datetime_to_apple_time

        # Get query embedding
        query_embedding = self.model.encode(query, convert_to_numpy=True).tolist()

        # Build query to get candidate messages
        sql = """
            SELECT message_id, chat_id, date, is_from_me, text, embedding
            FROM message_embeddings
            WHERE 1=1
        """
        params: list = []

        if chat_ids:
            placeholders = ",".join("?" * len(chat_ids))
            sql += f" AND chat_id IN ({placeholders})"
            params.extend(chat_ids)
        elif chat_id:
            sql += " AND chat_id = ?"
            params.append(chat_id)

        if after:
            sql += " AND date > ?"
            params.append(datetime_to_apple_time(after))

        if before:
            sql += " AND date < ?"
            params.append(datetime_to_apple_time(before))

        cursor = self.conn.execute(sql, params)

        # Calculate similarities and collect results
        results = []
        for row in cursor:
            embedding = _deserialize_embedding(row["embedding"])
            similarity = _cosine_similarity(query_embedding, embedding)

            if similarity >= min_similarity:
                results.append(SemanticSearchResult(
                    message_id=row["message_id"],
                    chat_id=row["chat_id"],
                    text=row["text"],
                    date=apple_time_to_datetime(row["date"]),
                    is_from_me=bool(row["is_from_me"]),
                    similarity=similarity,
                ))

        # Sort by similarity (highest first) and limit
        results.sort(key=lambda r: r.similarity, reverse=True)
        for result in results[:limit]:
            yield result

    def get_stats(self) -> dict:
        """Get statistics about the embedding index.

        Returns:
            Dictionary with index statistics
        """
        indexed_count = self._get_indexed_count()
        dimension = self._get_embedding_dimension()

        # Get model name from metadata
        cursor = self.conn.execute(
            "SELECT value FROM embedding_metadata WHERE key = 'model_name'"
        )
        row = cursor.fetchone()
        stored_model = row["value"] if row else None

        return {
            "indexed_messages": indexed_count,
            "embedding_dimension": dimension,
            "model_name": stored_model,
            "model_available": EMBEDDINGS_AVAILABLE,
            "index_path": str(self.path),
            "index_size_bytes": self.path.stat().st_size if self.path.exists() else 0,
        }

    def close(self) -> None:
        """Close the database connection."""
        if self._conn:
            self._conn.close()
            self._conn = None


def is_available() -> bool:
    """Check if embedding functionality is available.

    Returns:
        True if sentence-transformers is installed
    """
    return EMBEDDINGS_AVAILABLE


def get_available_models() -> list[dict]:
    """Get a list of recommended embedding models.

    Returns:
        List of model info dictionaries
    """
    return [
        {
            "name": "all-MiniLM-L6-v2",
            "size_mb": 90,
            "description": "Fast and efficient, good quality. Recommended for most uses.",
            "dimensions": 384,
        },
        {
            "name": "all-mpnet-base-v2",
            "size_mb": 420,
            "description": "Higher quality embeddings, slower. Best for accuracy.",
            "dimensions": 768,
        },
        {
            "name": "paraphrase-MiniLM-L3-v2",
            "size_mb": 60,
            "description": "Smallest and fastest. Good for limited resources.",
            "dimensions": 384,
        },
    ]
